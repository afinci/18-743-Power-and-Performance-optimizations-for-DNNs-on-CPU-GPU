In terms of the performance and power analysis for deep learning tasks, prior art focuses on characterizing the difference between various platforms running different deep learning workloads. Nurvitadhi et al \cite{nurvitadhi2016accelerating} compares the power/performance characteristics of binarized neural network on CPU, GPU, FPGA, and ASIC, which shows FPGA implementation have much better performance per watts compared to both CPU and GPU. Malik et al \cite{malik2016architecture} shows that GPU and FPGA can compete in energy delay product and depends on the input size, or the level of data parallelism.

This work obviously diverges from prior works since we focus on what tasks can be done on CPU while GPU is running deep learning workloads. In other words, since it is shown that GPU is almost always better than CPU in deep learning workloads, we want to understand the role for CPU in the big deep learning era.