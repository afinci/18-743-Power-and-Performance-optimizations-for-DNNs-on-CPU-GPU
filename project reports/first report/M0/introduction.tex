With the recent success of machine learning workloads, there are growing interests in understanding the speed/power consumption trade-offs of CPU, GPU, FPGA, and ASIC while running those intelligent workloads \cite{malik2016architecture, nurvitadhi2016accelerating}. However, there is no prior art that analyzes the workloads for CPU given that GPU is running a deep learning workload in the meantime. We find this question interesting and easily neglected since people focus on deep learning on GPU. Our argument is that even though deep learning task is considered the top priority task in the whole system, there is still a lot of CPU resources to utilize. For example, suppose the mobile device that equips with both CPU and GPU has to run deep learning model in the background all the time to analyze the context of user, we expect users to run applications in the meantime. However, it is not clear, given that both CPU and GPU share the same power and memory budget, what kind of tasks are allowed to utilize the CPU with affecting the performance of the deep learning task.

In this work, we try to understand and analyze what type of tasks, under what kind of frequencies can the CPU run under various GPU constraint. We will consider the deep neural network that run on GPU the top priority task of the overall system. Hence, the performance of it acts as the constraints in our analysis. We specifically focus on the embedded platform and we use Nvidia's Tegra X1 throughout our study. In embedded MPSoC where CPU-GPU are integrated on the same chip, it is shown that global power management that controls the frequency of both CPU and GPU is essential due to shared power budget \cite{pathania2014integrated} or even shares the same main memory. Hence, if one wants neither under-utilize the quad-core CPU nor performance degradation for the deep neural network, analysis is required to understand the sweet spot.

We study the 3 types of CPU benchmarks, 3 types of deep neural network, and different CPU-GPU frequency. For CPU, we mainly investigate the workloads in SPLASH2 benchmark suite \cite{woo1995splash} and try to come up with memory-intensive, compute-intensive, and mid-level workload in both compute and memory. For GPU, we target specifically on image classification task and includes three types of deep neural network from small to large. We also consider 3 different frequencies for CPU and 3 different frequencies for GPU.
