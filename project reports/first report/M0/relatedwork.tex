In terms of performance and power analysis for deep learning, prior arts focus on characterizing the different of different platform running deep learning workloads in terms of power/performance characteristics. Nurvitadhi et al \cite{nurvitadhi2016accelerating} compares the power/performance characteristics of binarized neural network on CPU, GPU, FPGA, and ASIC, which shows FPGA implementation have much better performance per watts compared to both CPU and GPU. Malik et al \cite{malik2016architecture} shows that GPU and FPGA can compete  in energy delay product and depends on the input size, or the level of data parallelism.

This work obviously diverges from their prior works since we focus on what are the tasks can be done on CPU while GPU is running deep learning workloads. In other words, since it is shown that GPU is almost always better than CPU in deep learning workloads, we want to understand the role for CPU in the big deep learning era.